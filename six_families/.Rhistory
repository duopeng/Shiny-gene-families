sample_idx = sample(nrow(df_var_subset),size=nrow(df_var_subset)*ratio,replace=FALSE)
df_tmp_validate = df_var_subset[sample_idx,]
df_tmp_train = df_var_subset[-sample_idx,]
#fit knnRes
knnRes <- knn(df_tmp_train[,-1],df_tmp_validate[,-1],df_tmp_train$response,k=kTmp)
tmpTbl <- table(df_tmp_validate$response,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
########################################################
# For final prediction                                 #
# Train random forest with all available training data #
########################################################
#only use vars that have MeanDecreaseGini>=10
idx = imp_sorted$MeanDecreaseGini>=10
vars_rf = rownames(imp_sorted)[idx]
cols_to_use = colnames(df_rf) %in% vars_rf
df_rf_var_subset = data.frame(response=df_rf[,1], df_rf[,cols_to_use])
test_errors = list()
df_error_rate = data.frame(value_of_performance_metrics=double(), split_ratio=character(), type=character())
df_sensi_speci_acc = data.frame(value_of_performance_metrics=double(), split_ratio=character(), type=character())
df_tmp_train = df_rf_var_subset
#fit
rfRes <- randomForest(df_tmp_train[,-1],df_tmp_train[,1])
#Out-of-bag error
OOB_err = mean(rfRes$err.rate[,1])
print(paste0("The Out-of-bag error rate is ", OOB_err))
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
View(test_Dat)
################
#Load test data#
################
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(response=test_Dat$response, id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
################
#Load test data#
################
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
View(test_Dat_Dummy)
test_Dat_Dummy = data.frame(response = test_Dat_Dummy$response, scale(test_Dat_Dummy[,-1]))
test_Dat_Dummy = data.frame(scale(test_Dat_Dummy[,-1]))
#drop the id column
test_Dat_Dummy=test_Dat_Dummy[,-1]
View(train_Dat_Dummy)
test_Dat_Dummy_covariates = test_Dat_Dummy[,1:4]
x2Tmp <- NULL
tmpCnms <- NULL
variable_names <- colnames(test_Dat_Dummy_covariates)
for ( iTmp in 2:ncol(test_Dat_Dummy_covariates) ) {
# multiply it by all other terms,
# excluding already generated pairwise combinations:
for ( jTmp in (iTmp):ncol(test_Dat_Dummy_covariates) ) {
x2Tmp <- cbind(x2Tmp,test_Dat_Dummy_covariates[,iTmp]*test_Dat_Dummy_covariates[,jTmp])
# maintain vector of column names for quadratic
# terms along the way:
tmpCnms <- c(tmpCnms,paste0(variable_names[iTmp],"_X_",variable_names[jTmp]))
}
}
# name attributes in the matrix of quadratic terms:
colnames(x2Tmp) <- tmpCnms
# add quadratic terms to dataframe
test_Dat_Dummy_withInteraction = data.frame(test_Dat_Dummy_covariates,x2Tmp)
View(test_Dat_Dummy_withInteraction)
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(response = test_Dat_Dummy_withInteraction[,1], test_Dat_Dummy_withInteraction[,cols_to_use])
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(response = test_Dat_Dummy_withInteraction[,1], test_Dat_Dummy_withInteraction[,cols_to_use])
interaction_vars_for_rf = c("sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "dtj_X_bw")
cols_to_use = colnames(df_tmp) %in% interaction_vars_for_rf
df_interactions = data.frame(response = df_tmp[,1], df_tmp[,cols_to_use])
test_df_rf = data.frame(test_Dat_Dummy, df_interactions[,-1])
test_Dat_Dummy_withInteraction
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(response = test_Dat_Dummy_withInteraction[,1], test_Dat_Dummy_withInteraction[,cols_to_use])
View(dfTmp)
View(df_tmp)
View(train_Dat_Dummy_covariates)
View(train_Dat_Dummy_withInteraction)
View(train_Dat_Dummy_covariates)
View(test_Dat_Dummy_covariates)
test_Dat_Dummy_covariates = test_Dat_Dummy[,1:4]
x2Tmp <- NULL
tmpCnms <- NULL
variable_names <- colnames(test_Dat_Dummy_covariates)
for ( iTmp in 1:ncol(test_Dat_Dummy_covariates) ) {
# multiply it by all other terms,
# excluding already generated pairwise combinations:
for ( jTmp in (iTmp):ncol(test_Dat_Dummy_covariates) ) {
x2Tmp <- cbind(x2Tmp,test_Dat_Dummy_covariates[,iTmp]*test_Dat_Dummy_covariates[,jTmp])
# maintain vector of column names for quadratic
# terms along the way:
tmpCnms <- c(tmpCnms,paste0(variable_names[iTmp],"_X_",variable_names[jTmp]))
}
}
# name attributes in the matrix of quadratic terms:
colnames(x2Tmp) <- tmpCnms
# add quadratic terms to dataframe
test_Dat_Dummy_withInteraction = data.frame(test_Dat_Dummy_covariates,x2Tmp)
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(response = test_Dat_Dummy_withInteraction[,1], test_Dat_Dummy_withInteraction[,cols_to_use])
View(df_tmp)
test_df_rf = data.frame(test_Dat_Dummy, df_interactions[,-1])
View(test_df_rf)
View(df_tmp)
View(test_Dat_Dummy)
View(test_Dat)
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
View(test_Dat)
View(test_Dat_Dummy)
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
View(train_Dat_Dummy)
test_Dat_Dummy
View(test_Dat_Dummy)
test_Dat_Dummy = data.frame(scale(test_Dat_Dummy))
#drop the id column
test_Dat_Dummy=test_Dat_Dummy[,-1]
View(test_Dat_Dummy)
########################
# add interaction terms#
########################
test_Dat_Dummy_covariates = test_Dat_Dummy[,1:4]
x2Tmp <- NULL
tmpCnms <- NULL
variable_names <- colnames(test_Dat_Dummy_covariates)
for ( iTmp in 1:ncol(test_Dat_Dummy_covariates) ) {
# multiply it by all other terms,
# excluding already generated pairwise combinations:
for ( jTmp in (iTmp):ncol(test_Dat_Dummy_covariates) ) {
x2Tmp <- cbind(x2Tmp,test_Dat_Dummy_covariates[,iTmp]*test_Dat_Dummy_covariates[,jTmp])
# maintain vector of column names for quadratic
# terms along the way:
tmpCnms <- c(tmpCnms,paste0(variable_names[iTmp],"_X_",variable_names[jTmp]))
}
}
# name attributes in the matrix of quadratic terms:
colnames(x2Tmp) <- tmpCnms
# add quadratic terms to dataframe
test_Dat_Dummy_withInteraction = data.frame(test_Dat_Dummy_covariates,x2Tmp)
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(response = test_Dat_Dummy_withInteraction[,1], test_Dat_Dummy_withInteraction[,cols_to_use])
View(df_tmp)
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw", "is_x_sci")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(test_Dat_Dummy_withInteraction[,cols_to_use])
View(df_tmp)
View(test_Dat_Dummy_withInteraction)
View(test_Dat_Dummy)
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
###########################
#scale and center the data#
###########################
test_Dat_Dummy = data.frame(scale(test_Dat_Dummy))
#drop the id column
test_Dat_Dummy=test_Dat_Dummy[,-1]
########################
# add interaction terms#
########################
test_Dat_Dummy_covariates = test_Dat_Dummy[,1:5]
x2Tmp <- NULL
tmpCnms <- NULL
variable_names <- colnames(test_Dat_Dummy_covariates)
for ( iTmp in 1:ncol(test_Dat_Dummy_covariates) ) {
# multiply it by all other terms,
# excluding already generated pairwise combinations:
for ( jTmp in (iTmp):ncol(test_Dat_Dummy_covariates) ) {
x2Tmp <- cbind(x2Tmp,test_Dat_Dummy_covariates[,iTmp]*test_Dat_Dummy_covariates[,jTmp])
# maintain vector of column names for quadratic
# terms along the way:
tmpCnms <- c(tmpCnms,paste0(variable_names[iTmp],"_X_",variable_names[jTmp]))
}
}
# name attributes in the matrix of quadratic terms:
colnames(x2Tmp) <- tmpCnms
# add quadratic terms to dataframe
test_Dat_Dummy_withInteraction = data.frame(test_Dat_Dummy_covariates,x2Tmp)
#################
#variable subset#
#################
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw", "is_x_sci")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(test_Dat_Dummy_withInteraction[,cols_to_use])
View(df_tmp)
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
###########################
#scale and center the data#
###########################
test_Dat_Dummy = data.frame(scale(test_Dat_Dummy))
#drop the id column
test_Dat_Dummy=test_Dat_Dummy[,-1]
########################
# add interaction terms#
########################
test_Dat_Dummy_covariates = test_Dat_Dummy[,1:5]
x2Tmp <- NULL
tmpCnms <- NULL
variable_names <- colnames(test_Dat_Dummy_covariates)
for ( iTmp in 1:ncol(test_Dat_Dummy_covariates) ) {
# multiply it by all other terms,
# excluding already generated pairwise combinations:
for ( jTmp in (iTmp):ncol(test_Dat_Dummy_covariates) ) {
x2Tmp <- cbind(x2Tmp,test_Dat_Dummy_covariates[,iTmp]*test_Dat_Dummy_covariates[,jTmp])
# maintain vector of column names for quadratic
# terms along the way:
tmpCnms <- c(tmpCnms,paste0(variable_names[iTmp],"_X_",variable_names[jTmp]))
}
}
# name attributes in the matrix of quadratic terms:
colnames(x2Tmp) <- tmpCnms
# add quadratic terms to dataframe
test_Dat_Dummy_withInteraction = data.frame(test_Dat_Dummy_covariates,x2Tmp)
#################
#variable subset#
#################
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw", "is_x_sci")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(test_Dat_Dummy_withInteraction[,cols_to_use])
test_df_rf = data.frame(test_Dat_Dummy, df_interactions[,-1])
#only use vars that have MeanDecreaseGini>=10
idx = imp_sorted$MeanDecreaseGini>=10
vars_rf = rownames(imp_sorted)[idx]
cols_to_use = colnames(test_df_rf) %in% vars_rf
test_df_rf_var_subset = data.frame(test_df_rf[,cols_to_use])
test_df_rf = data.frame(test_Dat_Dummy, df_tmp)
################
#Load test data#
################
test_Dat <- read.table("final-exam-data/final-data-test.csv",header = TRUE,sep=",")
###############
#preprocessing#
###############
##########################
#generate dummy variables#
##########################
dummies_tmp = data.frame(row.names = row.names(test_Dat))
for (variable in c("wc","zwp","wi","bnf","ent","ypz","tdt","sb","ox","xt","np", "ku"))
{
for(value in unique(test_Dat[[variable]])) {
dummies_tmp[paste(variable, value ,sep="_")] <- ifelse(test_Dat[[variable]]==value,1,0)
}
}
test_Dat_Dummy = cbind.data.frame(id=test_Dat$id, dtj=test_Dat$dtj, qh=test_Dat$qh, sci=test_Dat$sci, bw=test_Dat$bw, is=test_Dat$is, dummies_tmp)
###########################
#scale and center the data#
###########################
test_Dat_Dummy = data.frame(scale(test_Dat_Dummy))
#drop the id column
test_Dat_Dummy=test_Dat_Dummy[,-1]
########################
# add interaction terms#
########################
test_Dat_Dummy_covariates = test_Dat_Dummy[,1:5]
x2Tmp <- NULL
tmpCnms <- NULL
variable_names <- colnames(test_Dat_Dummy_covariates)
for ( iTmp in 1:ncol(test_Dat_Dummy_covariates) ) {
# multiply it by all other terms,
# excluding already generated pairwise combinations:
for ( jTmp in (iTmp):ncol(test_Dat_Dummy_covariates) ) {
x2Tmp <- cbind(x2Tmp,test_Dat_Dummy_covariates[,iTmp]*test_Dat_Dummy_covariates[,jTmp])
# maintain vector of column names for quadratic
# terms along the way:
tmpCnms <- c(tmpCnms,paste0(variable_names[iTmp],"_X_",variable_names[jTmp]))
}
}
# name attributes in the matrix of quadratic terms:
colnames(x2Tmp) <- tmpCnms
# add quadratic terms to dataframe
test_Dat_Dummy_withInteraction = data.frame(test_Dat_Dummy_covariates,x2Tmp)
#################
#variable subset#
#################
vars_to_use = c("qh","sci","dtj","sci_X_sci", "qh_X_qh", "dtj_X_dtj", "dtj_X_qh", "qh_X_bw", "qh_X_sci", "bw_X_bw", "sci_X_is", "bw", "dtj_X_bw", "is_x_sci")
cols_to_use = colnames(test_Dat_Dummy_withInteraction) %in% vars_to_use
df_tmp = data.frame(test_Dat_Dummy_withInteraction[,cols_to_use])
test_df_rf = data.frame(test_Dat_Dummy, df_tmp)
#only use vars that have MeanDecreaseGini>=10
idx = imp_sorted$MeanDecreaseGini>=10
vars_rf = rownames(imp_sorted)[idx]
cols_to_use = colnames(test_df_rf) %in% vars_rf
test_df_rf_var_subset = data.frame(test_df_rf[,cols_to_use])
df_rf
test_df_rf
save.image("C:/OneDrive_HSPH/OneDrive - Harvard University/courses/stat learning/final/12_17.RData")
prediction = predict(rfRes,newdata=test_df_rf_var_subset)
test_df_rf_var_subset
prediction
prediction_binary = prediction
prediction_binary[prediction==Y]=1
prediction_binary = prediction
prediction_binary[prediction=='Y']=1
prediction_binary[prediction=='N']=0
prediction_binary = as.character(prediction)
prediction_binary[prediction=='Y']=1
prediction_binary[prediction=='N']=0
prediction_binary
df_pred = data.frame(id=test_Dat$id,pred=prediction_binary)
View(df_pred)
prediction = predict(rfRes,newdata=test_df_rf_var_subset)
prediction_binary = as.character(prediction)
df_pred = data.frame(id=test_Dat$id,pred=prediction_binary)
View(df_pred)
prediction = predict(rfRes,newdata=test_df_rf_var_subset)
prediction_binary = as.character(prediction)
df_pred = data.frame(id=test_Dat$id, sohappyitsdone=prediction_binary)
###########################
#write predictions to file#
###########################
write.table(df_pred, file="sohappyitsdone.csv", sep=",")
###########################
#write predictions to file#
###########################
write.table(df_pred, file="sohappyitsdone.csv", sep=",", row.names = FALSE)
prediction = predict(rfRes,newdata=test_df_rf_var_subset)
prediction_binary = as.character(prediction)
df_pred = data.frame(id=test_Dat$id, sohappyitsdone=as.factor(prediction_binary))
###########################
#write predictions to file#
###########################
write.table(df_pred, file="sohappyitsdone.csv", sep=",", row.names = FALSE)
###########################
#write predictions to file#
###########################
write.table(df_pred, file="sohappyitsdone.csv", sep=",", row.names = FALSE, quote=FALSE)
library(neuralnet)
library(Metrics)
######################################
# find optimum node and layer number #
######################################
#only use vars that have MeanDecreaseGini>=500
idx = imp_sorted$MeanDecreaseGini>=500
vars = rownames(imp_sorted)[idx]
cols_to_use = colnames(df_rf) %in% vars
df_var_subset = data.frame(response=df_rf[,1], df_rf[,cols_to_use])
df_var_subset = df_var_subset[1:1000,]
# hidden = c(2,2)
nnRes <- neuralnet(response~.,df_var_subset,hidden=c(2,2),linear.output = TRUE ,err.fct="sse", stepmax = 1e+06)
#get training error and test error
training_err <- mse(nnRes$net.result[[1]][,1],nnRes$response) #calculate train MSE
df_test=data.frame(response=df_rf[,1], df_rf[,cols_to_use])[2001:10000,]
df_test$response = as.character(df_test$response)
df_test$response[df_test$response=="Y"]=1
df_test$response[df_test$response=="N"]=0
df_test$response = as.numeric(df_test$response)
test_err=mse(predict(nnRes,df_test[,-1]),df_test$response) #calculate test MSE
print(paste0("neural network with 2 layers with 2 nodes each layer, the training error is ", training_err, ", test error is ", test_err))
# hidden = c(2,4)
nnRes <- neuralnet(response~.,df_var_subset,hidden=c(2,4),linear.output = TRUE ,err.fct="sse", stepmax = 1e+06)
#get training error and test error
training_err <- mse(nnRes$net.result[[1]][,1],nnRes$response) #calculate train MSE
df_test=data.frame(response=df_rf[,1], df_rf[,cols_to_use])[2001:10000,]
df_test$response = as.character(df_test$response)
df_test$response[df_test$response=="Y"]=1
df_test$response[df_test$response=="N"]=0
df_test$response = as.numeric(df_test$response)
test_err=mse(predict(nnRes,df_test[,-1]),df_test$response) #calculate test MSE
print(paste0("neural network with 2 layers with 4 nodes each layer, the training error is ", training_err, ", test error is ", test_err))
# hidden = c(4,4)
nnRes <- neuralnet(response~.,df_var_subset,hidden=c(4,4),linear.output = TRUE ,err.fct="sse", stepmax = 1e+06)
#get training error and test error
training_err <- mse(nnRes$net.result[[1]][,1],nnRes$response) #calculate train MSE
df_test=data.frame(response=df_rf[,1], df_rf[,cols_to_use])[2001:10000,]
df_test$response = as.character(df_test$response)
df_test$response[df_test$response=="Y"]=1
df_test$response[df_test$response=="N"]=0
df_test$response = as.numeric(df_test$response)
test_err=mse(predict(nnRes,df_test[,-1]),df_test$response) #calculate test MSE
print(paste0("neural network with 4 layers with 4 nodes each layer, the training error is ", training_err, ", test error is ", test_err))
# hidden = c(6,6)
nnRes <- neuralnet(response~.,df_var_subset,hidden=c(8,8),linear.output = TRUE ,err.fct="sse", stepmax = 1e+06)
#get training error and test error
training_err <- mse(nnRes$net.result[[1]][,1],nnRes$response) #calculate train MSE
df_test=data.frame(response=df_rf[,1], df_rf[,cols_to_use])[2001:10000,]
df_test$response = as.character(df_test$response)
df_test$response[df_test$response=="Y"]=1
df_test$response[df_test$response=="N"]=0
df_test$response = as.numeric(df_test$response)
test_err=mse(predict(nnRes,df_test[,-1]),df_test$response) #calculate test MSE
print(paste0("neural network with 8 layers with 8 nodes each layer, the training error is ", training_err, ", test error is ", test_err))
shiny::runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production')
runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production')
library(rsconnect)
library(rsconnect)
deployApp()
?save
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata",version=2)
checkRdaFiles
checkRdaFiles()
resaveRdaFiles
library(tools)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata",version=2)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata", compress ="none",version=2)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata", compress ="auto",version=2)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata", compress =none,version=2)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata", compress =0, version=2)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata", compress = FALSE, version=2)
resaveRdaFiles("C:\\OneDrive_HSPH\\OneDrive - Harvard University\\twitter_crawl\\saved_data\\rt2020_01_06_23_06_36.Rdata", compress = NULL, version=2)
shiny::runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production')
runApp()
runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production')
shiny::runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production_ts_fullAlnDist_v2')
runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production_ts_fullAlnDist_v2')
runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production_ts_fullAlnDist_v2')
runApp('C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production_ts_fullAlnDist_v2')
setwd("C:/OneDrive_HSPH/OneDrive - Harvard University/research/Tarleton_Lab/clustering/production")
library(ggplot2)
library(RColorBrewer)
#load distance matrix
dat = read.delim(paste0("data/mucin_TcYC6",".fasta.mat"), header = FALSE, sep = " ", quote = "\"", dec = ".", fill = TRUE)
dat2 <- dat[,-1]
rownames(dat2) <- dat[,1]
colnames(dat2) <- dat[,1]
#load loc
loc = read.delim(paste0("data/mucin_TcYC6",".fasta.mat.loc.tab"), col.names=c("gene_ID","chr","start","end","strand","annotation"), header = FALSE, sep = "\t", quote = "\"", dec = ".", fill = TRUE)
rownames(loc) <- loc[,1]
loc$length=loc$end-loc$start
loc$geneID_contig_length_annotation=paste(loc$gene_ID,loc$chr,loc$length,loc$annotation,sep=' ')
#MDS
d <- as.dist(dat2)
mds.coor <- cmdscale(d)
colnames(mds.coor)=c("x","y")
plot_mat = merge(mds.coor,loc, by = "row.names" )
x_lims=c(min(plot_mat$x),max(plot_mat$x))
y_lims=c(min(plot_mat$y),max(plot_mat$y))
mds.coor
?cmdscale()
